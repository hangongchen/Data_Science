{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:52:40.533515Z",
     "start_time": "2024-12-10T21:52:40.519470Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:52:40.628855Z",
     "start_time": "2024-12-10T21:52:40.525860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of classes: 20\n",
      "Total documents: 19997\n"
     ]
    }
   ],
   "source": [
    "data_root = '20_newsgroups'\n",
    "\n",
    "folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]\n",
    "folders = sorted(folders)\n",
    "classes = folders\n",
    "class_to_index = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "print(\"Detected classes:\", classes)\n",
    "print(\"Number of classes:\", len(classes))\n",
    "\n",
    "if len(classes) != 20:\n",
    "    raise ValueError(\"The number of detected classes is not 20. Please check your dataset.\")\n",
    "\n",
    "data_paths = []\n",
    "labels = []\n",
    "for c in classes:\n",
    "    class_path = os.path.join(data_root, c)\n",
    "    files = os.listdir(class_path)\n",
    "    files = [f for f in files if os.path.isfile(os.path.join(class_path, f))]\n",
    "    for f in files:\n",
    "        data_paths.append(os.path.join(class_path, f))\n",
    "        labels.append(class_to_index[c])\n",
    "\n",
    "data_paths = np.array(data_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"Total documents:\", len(data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:52:40.635709Z",
     "start_time": "2024-12-10T21:52:40.629126Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.arange(len(data_paths))\n",
    "np.random.shuffle(indices)\n",
    "data_paths = data_paths[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:52:44.841049Z",
     "start_time": "2024-12-10T21:52:40.657145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of custom stop words: 300\n"
     ]
    }
   ],
   "source": [
    "#set 300 stopwords\n",
    "word_counts = Counter()\n",
    "for path in data_paths:\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        words = text.strip().split()\n",
    "        word_counts.update(words)\n",
    "\n",
    "most_common_300 = [w for w, _ in word_counts.most_common(300)]\n",
    "stop_words = list(most_common_300)\n",
    "\n",
    "print(\"Number of custom stop words:\", len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:52:44.846956Z",
     "start_time": "2024-12-10T21:52:44.846374Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        input='filename',\n",
    "        encoding='utf-8',\n",
    "        decode_error='ignore',\n",
    "        lowercase=True,\n",
    "        stop_words=stop_words,\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )),\n",
    "    ('clf', LinearSVC(random_state=42, max_iter=2000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:53:30.985625Z",
     "start_time": "2024-12-10T21:52:44.849251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrischen/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['apr', 'ax', 'cantaloupe', 'cmu', 'computer', 'cs', 'd', 'date', 'didn', 'distribution', 'doesn', 'don', 'edu', 'fri', 'gmt', 'god', 'host', 'i', 'id', 'lines', 'm', 'max', 'message', 'mon', 'news', 'newsgroups', 'nntp', 'organization', 'path', 'posting', 're', 'references', 'reply', 's', 'sender', 'srv', 'state', 'subject', 't', 'thu', 'tue', 'university', 've', 'writes', 'x', 'xref'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/harrischen/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['apr', 'ax', 'cantaloupe', 'cmu', 'computer', 'cs', 'd', 'date', 'didn', 'distribution', 'doesn', 'don', 'edu', 'fri', 'gmt', 'god', 'host', 'i', 'id', 'lines', 'm', 'max', 'message', 'mon', 'news', 'newsgroups', 'nntp', 'organization', 'path', 'posting', 're', 'references', 'reply', 's', 'sender', 'srv', 'state', 'subject', 't', 'thu', 'tue', 'university', 've', 'writes', 'x', 'xref'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/harrischen/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['apr', 'ax', 'cantaloupe', 'cmu', 'computer', 'cs', 'd', 'date', 'didn', 'distribution', 'doesn', 'don', 'edu', 'fri', 'gmt', 'god', 'host', 'i', 'id', 'lines', 'm', 'max', 'message', 'mon', 'news', 'newsgroups', 'nntp', 'organization', 'path', 'posting', 're', 'references', 'reply', 's', 'sender', 'srv', 'state', 'subject', 't', 'thu', 'tue', 'university', 've', 'writes', 'x', 'xref'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/harrischen/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['apr', 'ax', 'cantaloupe', 'cmu', 'computer', 'cs', 'd', 'date', 'didn', 'distribution', 'doesn', 'don', 'edu', 'fri', 'gmt', 'god', 'host', 'i', 'id', 'lines', 'm', 'max', 'message', 'mon', 'news', 'newsgroups', 'nntp', 'organization', 'path', 'posting', 're', 'references', 'reply', 's', 'sender', 'srv', 'state', 'subject', 't', 'thu', 'tue', 'university', 've', 'writes', 'x', 'xref'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/harrischen/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['apr', 'ax', 'cantaloupe', 'cmu', 'computer', 'cs', 'd', 'date', 'didn', 'distribution', 'doesn', 'don', 'edu', 'fri', 'gmt', 'god', 'host', 'i', 'id', 'lines', 'm', 'max', 'message', 'mon', 'news', 'newsgroups', 'nntp', 'organization', 'path', 'posting', 're', 'references', 'reply', 's', 'sender', 'srv', 'state', 'subject', 't', 'thu', 'tue', 'university', 've', 'writes', 'x', 'xref'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(pipeline, data_paths, labels, cv=skf, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:53:30.996952Z",
     "start_time": "2024-12-10T21:53:30.991395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross-validation scores: [0.94025    0.94325    0.93873468 0.93848462 0.93473368]\n",
      "Mean accuracy: 0.9390905976494125\n"
     ]
    }
   ],
   "source": [
    "print(\"5-fold cross-validation scores:\", cv_scores)\n",
    "print(\"Mean accuracy:\", np.mean(cv_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
